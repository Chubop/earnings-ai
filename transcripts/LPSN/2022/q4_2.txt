Robert LoCascio
Thanks, John. Obviously, we took the opportunity. There's a lot of noise and a lot of puts and takes, but we put the past in a box, and now we have a foundation come Q2. And what I want to do is talk about what's next and what do we build from that foundation based on our strategy around AI that we've been pursuing for over five years. What I want to do is first talk about what our strategy is and address it from really three perspectives. One is a strategic perspective, an engineering product perspective and an industry perspective around our view of AI and how we're going about it.
From a strategic perspective, AI has two foundational pieces. One is the data models. And that's the stuff we're seeing with OpenAI and large language models. And the other, just as important, is actually the data set that the model uses to generate outcomes. There's been a lot of focus on the model because of what's gone on with ChatGPT. But we can see sometimes it has issues because the underlying data set is using public data. LivePerson has one of the largest and most precise conversational business data sets in the world. And I use the word precision because over 350,000 people on our platform each day, and they're messaging their live agents and they are generating 1 billion conversations a year, and these are high-quality conversations around many different topics, but the conversations are precise. There are answers to questions in the way the brand would want it answered. These agents, they create a long tail of answers.
Today, as we said in the past, we automate fully about 20% to 30% of the conversations that are on our platform. About 75% of conversations have some form of AI, but end-to-end automation is about 20% to 30%. With what's happened with large language models and what we're doing with them is that we can go after a much larger pool of intents. We can go after the long tail of intents that's sitting in our data today, and we believe we can automate 80% to 90% of conversations, not only on messaging, but in voice. We are bringing our voice AI product. It's weeks away now, and we're basically going to be able to go after both channels. So we sit in a very unique space to power the enterprise at scale and in a safe way with large language models, and we are going to accelerate what we're doing.
Now from an engineering perspective, we began developing our own AI capabilities 5 years ago. And today, 50% of our engineering budget is on AI and automation. And this is backed by world-class team of product and engineering leads, data scientists and many who came from the acquisitions we did over the past several years, from BotCentral, from VoiceBase, Tenfold, [MC], WildHealth. As a matter of fact, we recently took one of the founders of BotCentral, who we acquired 5 years ago, and now he's leading a group that is working with our largest top 100 enterprise customers so that they will adopt this technology at a faster rate. Because what we're hearing from our customer base is they want to talk. They want to talk to engineers, they want to talk to the product heads because they're going to buy AI from those people. And even our sales team, although they're out there and they're looking for more new opportunities, our base of customers want the implementation, want the knowledge from the people who built these platforms. So I think the impact of that on our net retention rates and also our ability to grow the company at a different level will be profoundly changed with a restructuring of that business.
And then lastly is, from an industry perspective, many of you may know, about four years ago, I founded a nonprofit called EqualAI with Miriam Vogel. Miriam was the Associate Deputy Attorney General under President Obama, and she led the team that developed the implicit bias training programs for federal law enforcement. She recently also has been appointed as a chairperson to President Biden's National AI Advisory Committee. And her and I built this nonprofit in this organization. We have a great Board of Directors and it's focused on one thing: How do you bring AI safely to the enterprise? And there's a framework and training that Miriam has developed with her team, and LivePerson has been a great supporter of that and we've obviously adopted many of those frameworks in our own technology. So what this all boils down to is that we're not trying to catch the future of AI. We're actually leading that and we're leading in our area, which is the enterprise AI space.
What I want to do is bring Joe Bradley on, who's been with us for five years. Joe is our Chief Data Scientist, and I want him to do a deep dive on our platform, our data set, what we're doing and what we've been doing with large language models and then some of the things that are happening recently that will also drive more profitability to the company. Just a little background before I bring Joe on, before he was here, he was Head of Data Science at Amazon Search and Nike, in addition to spending many years as a physicist and research associate at Lawrence Livermore National Laboratory, and he holds a doctorate in physics. So Joe leads our team of world-class scientists. And with that, Joe, let me turn it over to you. I think it's the first time that a data scientist has been on a quarterly call, but this is an AI-focused thing. So enjoy.
Joe Bradley
Thanks, Rob. Very happy to be here. I'll put up some visuals here real quick for people. The opportunities that the latest generation of generative AI and large language model technology creates for LivePerson are exciting. Essentially, they open up, as Rob mentioned, a large new addressable market for us. I'm here today to explain a little bit about how that works.
Essentially, LivePerson has three core strategic assets for machine-learned model building in general, but which are particularly relevant for this new generation of large language models. First, we have billions of conversations running through the platform. These are what we call goal-oriented dialogues, which just means people solving real problems and they're full of the real complexities of doing that with human dialogue. This is one of the world's richest baseline data sets for an LLM-based conversational system.
Second, we already have a rich processing layer for understanding that data, which creates billions of derived data points, including sentiment, conversation quality, user intent, user problem resolution, customer satisfaction, et cetera, much of this using prior and current generations of large language model technology already. It's part of why we have hundreds of millions of conversations handled today by our own first-party AI products. Third, we have, as Rob mentioned, over 300,000 human experts that log into the platform every day, providing feedback and helping models learn.
Each of these assets is uniquely meaningful in this new LLM context. First, these large language models need baseline data to train their basic behavior. This must be -- is typically done with and really must be done with, today, Internet-scale data sets. LP holds one of the few goal-oriented conversational data sets at this kind of scale. Secondly, because each conversation on LP's data set is already adorned with quality measures, associated business outcomes, et cetera, we can teach these models how to drive towards these outcomes by highlighting particularly successful conversations or by rewarding models differently as the conversation changes.
Third, in addition to the scale of the latest generation of large language models, the other key performance driver has become increasingly human feedback through techniques called reinforcement learning. The largest models in the world are typically trained with tens of thousands or hundreds of thousands of examples of such feedback, and our platform generates this scale of data daily. This is a powerful mechanism for controlling model behavior. It teaches them,- for example, when they might be hallucinating. It teaches them what not to say and when not to say it. This is incredibly important in an industrial context.
And this -- for all these reasons, this is why we're partnering with the world's leading AI organizations in the large language model and generative AI space. In particular, we're very excited about the global rollout possibilities that we're seeing in working with Microsoft's Azure platform. Our assets allow us to move quickly to produce high-precision conversational systems today and to further push the state of the art tomorrow. More importantly, they allow us to keep these models grounded, factual and aligned with the humans who will use them. What this means for us and for the world, I think, is that LLMs are transitioning from fit-for-purpose models that can understand aspects of the conversation and make specific decisions, into systems that themselves can drive and control a dialogue. We've all started to experience that with some of the latest ChatGPT technology, for instance.
That's a big change for us. Up until now, dialogues had to be programmed explicitly in our system and in any thought building system in the world. And this doesn't scale because dialogues are, by nature, a very long tail process. They're very unique. But we now have technology that's capable of handling dialogues in their full complexity and nuance. This is a critical development because it opens up a new scale for conversational automation and essentially a much bigger TAM for our products. Today, still, approximately 80% of business-to-consumer conversations occur over the phone. And I think none of us are satisfied with the IVR-style automation presented in that context today.
One of the main drivers of this problem and this dissatisfaction is this concept that long -- that dialogues are a long-tailed set of events. They are extremely unique. And existing automations can't handle all that different uniqueness without a massive code to effort -- without massive effort to code and maintain. Not only do the latest LLMs unlock this set of conversations and over time, will unlock more conversations, over time, they'll unlock more conversations in other enterprise use cases as well. Also, as we'll see in a moment with Matt's demonstration, opportunities open up outside the enterprise altogether, like the health domain.
But what do we mean by long tail, I think, is worth diving into that just for a minute. If that's the real problem here, what does that mean? It means that most dialogues don't -- are unique and don't fit into a tidy box. The easiest way to understand this is to look at some real brands and some real conversations.
For example, we have a major retailer who has millions of conversations on LivePerson annually, has a 20% automation rate. Here's a conversation that's a real conversation on their platform about scheduling a delivery. We all imagine scheduling a delivery to be kind of a simple task and kind of repetitive. You fill out a form and you give it your address. But here, you can see this person is trying to figure out if they can coordinate this delivery with a kitchen remodel, and they don't even know when the kitchen remodel is going to be finished. They want to make sure their appliance is going to show up on time and be hooked up properly and all the things that you and I would care about. Up until now, this would have resulted in a human escalation or programming some bespoke flow into a chatbot with a host of parameters that had to be managed. Now this is automatable.
One more example, a major bank and a customer simply trying to pay their mortgage. Again, we think of this as a simple, repetitive event. But as you can see here, it's often not. The customer in this case has had a payment increase that they were unaware of. They've been made a promise by a previous agent that they claim. With if you take large language models, you combine them with the LP data set, this claim becomes an auditable fact, for example, and the brand's policy on matters like this can be inferred from the data and executed by a machine. Incidentally, we worked with this particular brand to use the latest generation of generative AI to recognize thousands of intents like this with greater than 90% accuracy, which up until now have been unheard of.
One of the more exciting aspects of all of this is that to build systems -- the effort to build systems like this is radically reduced as well. When we bring together brand knowledge bases, brand conversational data in our platform with large language models, we can have a working system up and running with minutes of effort rather than weeks or months. In fact, over the last 6 to 8 weeks, we've built over 200 conversational AI systems in this way, and we've begun demonstrating this to some of our largest customers already. You'll see examples of systems that can handle this level of depth and flexibility of conversation shortly in Matt's WildHealth demonstration. And that's what I've got for now. So thanks, Rob, and thanks, everyone, for listening.
Robert LoCascio
Thanks, Joe, for your insights into what we're doing with our AI and using large language models and beyond that. I want to now bring Dr. Matt Dawson to speak about our AI health care initiatives. As many of you know, health care is a $4 trillion industry, or I'll say, problem. And alone, I think this market is ripe for transformation by AI. As such, we acquired WildHealth last year, brought in our health care offerings. And Dr. Dawson is WildHealth's Co-Founder and CEO. He's -- a little background on him. He's a published scientist, accomplished entrepreneur, started 5 or 6 businesses. He's won National Awards for innovation. Additionally, he developed a medical education app and podcast which touts millions of listeners and users.
And WildHealth, the reason we bought them is they're based on an AI platform called Clarity and which analyzes millions of data points from a patient's DNA, their blood markers, their gut, the biodata of wearables. And then they correlate that using machine learning, then give outcomes that are better than anything that's been out in the market before, things like diabetes. I mean they're solving diabetes on the scale that has not been seen before. And Matt will explain how the machine-based portion of bringing that data together, injected into a large language model, how it can change the trajectory of a broken business model of health care, and allow health care to be delivered to millions of people in a way that is radically different because precision health up to now was really for athletes. So we have some of the biggest professional athletes on our platform and CEOs and people like that. But we can deliver that level of accuracy and precision in health care to everybody.
So with that, I'll turn it over to Matt to give us some perspective and show us a little bit of the platform.
Matthew Dawson
Thanks, Rob. So I'm really excited to show this technology and how it illustrates some of those points that Joe mentioned before about the long tail. But first, I want to quickly explain the health care vision and big picture. Our vision, WildHealth and LivePerson together, is to take the highest quality health outcomes in the world and scale that to millions of people. We're already creating those outcomes because we have the most precise and personalized data in the world.
As Rob mentioned, we built the world's first and only true AI-driven precision medicine platform, called Clarity. And now Clarity in and of itself is really a game-changing technology, as it combines potentially millions of data points from 700,000 unique genes and blood biomarkers, microbiome data, phenotypic data, subjective feedback from patients and even data from wearables. And it takes all of that precise data and it generates a comprehensive report on how to fully optimize someone's health. So this is really a blueprint or a personalized playbook for an individual. It shows the specific perfect diet for them, the perfect exercise program for them, what supplements will work for them, what medications may or may not work for them, and even what diseases they may be most at risk for in the future and what are we going to do to decrease that risk.
So as Rob mentioned, this has led to incredible results such as reversing diabetes and prediabetes in 48% of our patients who have those disorders. You can compare that to 3% of patients in traditional medicine who have their diabetes reversed because of the smaller amount of data that they have, 3% versus 48% because of the personalized precision data. Now right now, this is delivered by humans. You have doctors and health coaches combined with the precision data, which is actually really similar to how customer engagement LivePerson uses data, but with humans in the loop, constantly improving it. The difficulty, as you can imagine, is the cost of the humans and the ability to scale them. So just like what LivePerson is doing in customer engagement, we can totally change the game on the scale of what a doctor and health coach can do. So as normally a doctor may have 1,000 patients, this data combined with LLMs can massively increase that ratio of patient to doctor, and at the same time, we radically change the margins on the health care business from maybe the 20% to 25% range to be more like platform margins while improving outcomes.
But instead of just telling you, I actually want to show you a demo of how we can do this right now, how we can take our rich precision data to train a large language model with all of our precision medicine knowledge and all the patient-specific data to get really incredible results. The model here that you're looking at is trained with my data, so all of my DNA, blood work, microbiome data, the millions of data points about me, and I can ask it questions. So just like the long tail discussion Joe was talking about, I can essentially take this conversation wherever I want based on what's important to me personally.
So for example, let's say someone in my family recently had a heart attack. Well, right now, I'm young and healthy. I don't have any real medical problems, but I want to look into the future. I would love to have a crystal ball where I can look and see what my risk factors are. So I can ask this model that has all of that genetic data and all the data about me, am I at risk of having a heart attack? And while I'm thinking about this, I'm going to go ahead and ask, and what about getting cancer and dementia? So those are things that run in my family as well that I'm worried about in the future. And I feel fine now, but these are things that kill most Americans. So as just looking into the crystal ball of all of my risk factors, the DNA, the blood work, it tells me, great news, I have a 0% risk of having a heart attack in the next 10 years based on the MACE score. However, not great news when I look and see, I've got an increased risk of dementia, have an increased risk of colon cancer here. And then as I go on down, it says based on my genetics, I have an increased risk of late onset Alzheimer's with sleep disturbance, if I have that. So that reminds me, I haven't been sleeping well recently, so any recommendations based on my genetics. I'm not asking it, just give me sleep tips. I'm saying look at my DNA, which also, while I'm asking, I want to go and ask what labs can I improve to decrease my chance of getting those diseases as well?
Again, this is about me, specifically me, reporting to my DNA, my labs, what can I do, not just the risk factors that I'm at risk for, but what am I going to do to decrease those risks? And I already have a specific example there for when it comes to sleep. So it tells me about my increased risk for Alzheimer's disease or sleep disturbance, but it gave me very specific things to do. It says avoid eating when it's dark, you need to fast for 12 hours, and it also says I have a very interesting clock polymorphism here now in my DNA. It's been associated with "night owl" [snip]. So maybe I should go to bed a little later and get up a little later, maybe adjust my schedule, my work schedule. And then for labs, the same thing. I should focus on LDL, ApoB levels, okay, great, tell me other things to do, how to do that. And then when it gets to dementia, it says to decrease your chance of getting dementia you should focus on improving your omega-3 levels. So I think I remember my omega-3 level was off in my last check, but I'm just going to ask the model, what is my omega-3 level? Because remember, it has all of my data, those potentially millions of data points. And while I'm thinking about that, okay, it is low here. So then the next question obviously is, should I take a supplement for this? And what other supplements should I take? Why stop at omega-3, it has all of my data. What should I be doing specifically for me? It says, yes, I should be taking an omega-3 supplement, and I'm going to take it. I'm motivated right now. I don't want to get dementia and have these diseases.
And as far as other supplements, it looks like vitamin D, zinc to reduce DOMS, okay, well, what is DOMS. This has all of the Internet's information, not just my specific information. And now I'm thinking, I don't have an omega-3 supplement, but I'm going to have dinner, so what foods are high in omega-3. And I see here that DOMS stands for delayed onset muscle soreness, good to know. I think fish are high on omega-3. There may be some other things, why guess, let's just ask the model. Great example of the long tail we talked about. And it says, yes, salmon, mackerel. I love salmon. So give me a good salmon recipe, right? I'm not going to ask my doctor this, but also give me a shopping list.
Again, I could go on and on with this AI. And in fact, I have. I've played with this an incredible amount because it's a wealth of information. I can learn so much about myself, like what to eat, how to exercise, my risk factor for scary diseases, and the conversational rabbit holes of the long tail, like Joe mentioned, that I can go down that we expect to really drive increased platform volumes for LivePerson. This is basically like having a doctor in my pocket, except it's one that knows all of my DNA, my blood work, my history, everything about me, and it's not going to get irritated when I ask for a salmon recipe and shopping list to go along with it.
Just to be really clear, though, the way that this is going to work initially is that there's going to be a human between the AI and the patient, so the patient can ask a question or a provider can ask a question of the AI as well. And the answer is going to come back, and then that provider decides whether to send it to the patient or to edit it and send it to the patient, but in either instance, it's going to be providing feedback to the model and further training for the model as well as giving this recommendation to the patient. So you can see it's going to be not just a massive upgrade on the quality of recommendations that truly personalize to the level of DNA, but also to efficiency. It's going to be a massive time savings for a doctor who doesn't need to look up someone's omega-3 level, look up a supplement, calculate a 10-year heart attack risk and, of course, give a salmon recipe with a shopping list. We're going to let the AI do all of those tasks with the incredible data sets and algorithms that it already has.
We know that these large language models have great potential. That's clear. But if combined with extremely rich data, this precision data, then we have something truly transformative. It should be in the hands of every doctor and every patient in the world. And this combination of precision data and large language models is going to transform the health of millions of people. Thank you, Rob.
